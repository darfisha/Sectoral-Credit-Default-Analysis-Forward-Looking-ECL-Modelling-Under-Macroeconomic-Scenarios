# -*- coding: utf-8 -*-
"""xav_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Og4WnbKtSr036xnp1dfJAtJNJtESrNmL
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight

import os
import gdown
import pandas as pd

file_id = "1MVW1amhh9k3ksDsJkRo9ELvEwRplG0r2"
url = f"https://drive.google.com/uc?id={file_id}"
output = "credit_risk.csv"

if not os.path.exists(output):
    print("Downloading file...")
    gdown.download(url, output, quiet=False)
else:
    print(f"File '{output}' already exists. Skipping download.")

# Explicit column names as seen in dataset
date_cols = [
    "Agreement Signing Date",
    "Board Approval Date",
    "Closed Date (Most Recent)",
    "Effective Date (Most Recent)",
    "First Repayment Date",
    "Last Disbursement Date",
    "Last Repayment Date",
]

df = pd.read_csv(output, parse_dates=date_cols, low_memory=False)

print("\nParsed dataset shape:", df.shape)
print("Parsed date columns:", [c for c in date_cols if c in df.columns])
print(df[date_cols].head())

df.info()

df.columns = [
    col.strip().lower().replace(" ", "_").replace("/", "_").replace("(", "").replace(")", "").replace("$", "usd").replace("'", "").replace(".", "")
    for col in df.columns
]

india_df = df[df['country___economy'].str.strip() == 'India'].copy()
india_df.shape
india_df.to_csv('india_credit_risk.csv', index=False)

india_df.info()

# 'currency_of_commitment' has all NaN
if 'currency_of_commitment' in india_df.columns:
    india_df.drop(columns=['currency_of_commitment'], inplace=True)

# Convert date columns
date_cols = [
    'end_of_period',
    'first_repayment_date',
    'last_repayment_date',
    'agreement_signing_date',
    'board_approval_date',
    'effective_date_most_recent',
    'closed_date_most_recent',
    'last_disbursement_date'
]

for col in date_cols:
    if col in india_df.columns:
        india_df[col] = pd.to_datetime(india_df[col], errors='coerce')

# Add origination year
india_df["origination_year"] = india_df["agreement_signing_date"].dt.year.astype("Int64")

india_df["origination_year"].unique()

#4. Financial columns
# -----------------------
numeric_cols = [
    'interest_rate', 'original_principal_amount_ususd', 'cancelled_amount_ususd',
    'undisbursed_amount_ususd', 'disbursed_amount_ususd', 'repaid_to_ibrd_ususd',
    'due_to_ibrd_ususd','exchange_adjustment_ususd',
    'borrowers_obligation_ususd', 'loans_held_ususd'
]

# Ensure numeric
india_df[numeric_cols] = india_df[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Remove negative values ‚Üí set as NaN
for col in numeric_cols:
    india_df[col] = india_df[col].apply(lambda x: np.nan if x < 0 else x)

# 5. Categorical cleanup
# -----------------------
for col in india_df.select_dtypes(include="object").columns:
    india_df[col] = india_df[col].astype(str).str.strip()

india_df['loan_status'] = india_df['loan_status'].astype('string').str.strip().str.upper()
print("Loan status values:", india_df['loan_status'].unique())

india_df['loan_status'] = (
    india_df['loan_status']
    .astype('string')   # Pandas nullable string dtype
    .str.strip()
    .str.upper()
)

print(india_df['loan_status'].unique())

active_statuses = [
    'REPAYING','DISBURSED','DISBURSING','DISBURSING&REPAYING',
    'FULLY DISBURSED','FULLY TRANSFERRED','APPROVED','SIGNED','EFFECTIVE'
]

india_df["is_active"] = india_df["loan_status"].isin(active_statuses).astype(int)

def encode_default_balanced(status, disbursed_amount):
    if not isinstance(status, str):
        return 1  # unknown ‚Üí risky by default

    status = status.strip().upper()

    #  Non-default
    if status in ["FULLY REPAID", "SIGNED", "APPROVED", "DISBURSING"]:
        return 0

    #   Default (risky or incomplete repayment)
    if status in ["REPAYING", "DISBURSED", "DISBURSING&REPAYING", "FULLY DISBURSED"]:
        return 1

    # Special case: CANCELLED
    if status in ["CANCELLED", "FULLY CANCELLED"]:
        if disbursed_amount and disbursed_amount > 0:
            return 1  # risky cancel
        else:
            return 0  # safe cancel (no money given)

    # Catch-all: treat as risky
    return 1


# Apply to dataframe
india_df["default_flag"] = india_df.apply(
    lambda row: encode_default_balanced(row["loan_status"], row["disbursed_amount_ususd"]),
    axis=1
)

print("Default flag distribution (balanced+disbursed check):")
print(india_df["default_flag"].value_counts(normalize=True) * 100)

stress_df = pd.read_excel('/content/GDP+CPI DATA.xls')
stress_df.head()

# Transpose for yearly format
stress_long = stress_df.set_index("Indicator Name").T.reset_index()
stress_long.columns = ["year", "gdp_growth", "cpi_inflation"]
stress_long["year"] = stress_long["year"].astype(int)
print(stress_long.head())

# 8. Aggregate yearly credit risk
# -----------------------
india_df["year"] = india_df["agreement_signing_date"].dt.year

# Merge on year
merged_df = pd.merge(india_df, stress_long, on="year", how="inner")

# üî• Filter  of 2014‚Äì2024
merged_df = merged_df[(merged_df["year"] >= 2014) & (merged_df["year"] <= 2024)]

print("Filtered merged dataset shape:", merged_df.shape)
print(merged_df.head(5))

merged_df.drop(columns=[
    "region", "country___economy", "borrower", "guarantor_country___economy_code",
    "guarantor", "sold_3rd_party_ususd",
    "repaid_3rd_party_ususd", "due_3rd_party_ususd", "agreement_signing_date",
    "closed_date_most_recent", "board_approval_date", "last_disbursement_date"
], inplace=True)

merged_df.shape

# -----------------------
# 1. Filter dataset for 2014‚Äì2024
# -----------------------
mask = (india_df["year"] >= 2014) & (india_df["year"] <= 2024)
subset_df = india_df[mask].copy()

# -----------------------
# 2. Count loan statuses
# -----------------------
status_counts = subset_df["loan_status"].value_counts(dropna=False)
status_perc = (status_counts / status_counts.sum() * 100).round(2)

print("Loan status distribution (2014‚Äì2024):")
print(status_counts)
print("\nLoan status distribution (%) (2014‚Äì2024):")
print(status_perc)

# -----------------------
# 3. Cross-check with repayment/disbursement
# -----------------------
summary = subset_df.groupby("loan_status")[["disbursed_amount_ususd", "repaid_to_ibrd_ususd"]].agg(
    {"disbursed_amount_ususd": "mean", "repaid_to_ibrd_ususd": "mean"}
)

print("\nAverage disbursed vs repaid per status (2014‚Äì2024):")
print(summary)

# Count defaults (1) vs non-defaults (0)
default_counts = merged_df["default_flag"].value_counts(dropna=False)

print("Number of defaults vs non-defaults:")
print(default_counts)

# Percentage breakdown
print("\nPercentage distribution:")
print((default_counts / default_counts.sum() * 100).round(2))

merged_df.columns

merged_df.head()

# -----------------------
# Loan-to-GDP Ratio
merged_df["loan_to_gdp_growth_ratio"] = (
    merged_df["original_principal_amount_ususd"] / (merged_df["gdp_growth"] * 1e9)
).replace([np.inf, -np.inf], np.nan).fillna(0)


merged_df["repayment_ratio"] = (
    merged_df["repaid_to_ibrd_ususd"] / merged_df["disbursed_amount_ususd"]
).replace([np.inf, -np.inf], np.nan).fillna(0)

# -----------------------
# Check final engineered features
# -----------------------
print(merged_df[[
    "year", "default_flag", "repayment_ratio",
"loan_to_gdp_growth_ratio"
]].sample(10))

#EDA
missing_percent = merged_df.isnull().mean() * 100
missing_percent.sort_values(ascending=False)

merged_df['default_flag'].value_counts(normalize=True)
sns.countplot(x='default_flag', data=india_df)

# All numeric financial features + engineered ones
numeric_cols = [
    'interest_rate', 'original_principal_amount_ususd', 'cancelled_amount_ususd',
    'undisbursed_amount_ususd', 'disbursed_amount_ususd', 'repaid_to_ibrd_ususd',
    'due_to_ibrd_ususd','exchange_adjustment_ususd',
    'borrowers_obligation_ususd', 'loans_held_ususd',
    "repayment_ratio", "loan_to_gdp_growth_ratio",
]

sns.boxplot(data=merged_df[numeric_cols])

plt.figure(figsize=(12,8))
sns.heatmap(merged_df[numeric_cols + ['default_flag']].corr(), annot=True, cmap='coolwarm')
plt.show()

default_trend = merged_df.groupby('year')['default_flag'].mean()
default_trend.plot(kind='line')
plt.ylabel('Default Rate')
plt.title('Default Rate Over Years')
plt.show()

sns.scatterplot(x='gdp_growth', y='loan_to_gdp_growth_ratio', hue='default_flag', data=merged_df)

sns.histplot(data=merged_df, x='repayment_ratio', hue='default_flag', kde=True)

# -----------------------
# Chronological split (NO shuffle)
# Train: 2014‚Äì2020, Val: 2021‚Äì2022, Test: 2023‚Äì2024
# -----------------------
train_df = merged_df[(merged_df["year"] >= 2014) & (merged_df["year"] <= 2020)].copy()
val_df   = merged_df[(merged_df["year"] >= 2021) & (merged_df["year"] <= 2022)].copy()
test_df  = merged_df[(merged_df["year"] >= 2023) & (merged_df["year"] <= 2024)].copy()

# Extract project IDs and names from your split DataFrames
train_ids   = train_df["project_id"].values
train_names = train_df["project_name"].values

val_ids     = val_df["project_id"].values
val_names   = val_df["project_name"].values

test_ids    = test_df["project_id"].values
test_names  = test_df["project_name"].values


def show_split_info(name, df):
    if df.empty:
        print(f"{name}: EMPTY")
        return
    print(f"\n{name}: shape={df.shape}, years={int(df['year'].min())}‚Üí{int(df['year'].max())}")
    print("  class %:", (df["default_flag"].value_counts(normalize=True)*100).round(2).to_dict())

show_split_info("Train", train_df)
show_split_info("Val",   val_df)
show_split_info("Test",  test_df)

# Separate features & target
# -----------------------
X_train = train_df[numeric_cols].values
y_train = train_df["default_flag"].values

X_val   = val_df[numeric_cols].values if not val_df.empty else np.empty((0, len(numeric_cols)))
y_val   = val_df["default_flag"].values if not val_df.empty else np.array([])

X_test  = test_df[numeric_cols].values if not test_df.empty else np.empty((0, len(numeric_cols)))
y_test  = test_df["default_flag"].values if not test_df.empty else np.array([])

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")
X_train_imputed = imputer.fit_transform(X_train)
X_val_imputed   = imputer.transform(X_val)
X_test_imputed  = imputer.transform(X_test)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_val_scaled   = scaler.transform(X_val_imputed)
X_test_scaled  = scaler.transform(X_test_imputed)

print("\nScaled shapes:",
      "\n  X_train:", X_train_scaled.shape,
      "\n  X_val  :", X_val_scaled.shape,
      "\n  X_test :", X_test_scaled.shape)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# ------------------------
# Impute + Scale
# ------------------------
imputer = SimpleImputer(strategy="mean")

X_train_imputed = imputer.fit_transform(X_train)
X_val_imputed   = imputer.transform(X_val) if X_val.size else X_val
X_test_imputed  = imputer.transform(X_test) if X_test.size else X_test

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train_imputed)
X_val_scaled   = scaler.transform(X_val_imputed) if X_val_imputed.size else X_val_imputed
X_test_scaled  = scaler.transform(X_test_imputed) if X_test_imputed.size else X_test_imputed

print("\nFinal Shapes:",
      "\n  X_train:", X_train_scaled.shape,
      "\n  X_val  :", X_val_scaled.shape,
      "\n  X_test :", X_test_scaled.shape)

# ------------------------
# Then re-run your models loop here
# ------------------------

!pip install catboost

"""Before Tuning"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd

# ------------------------
# Define models with tuned hyperparameters
# ------------------------
models = {
    "Logistic Regression": LogisticRegression(
        class_weight="balanced", solver="lbfgs", C=1.0, max_iter=2000, random_state=42
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=300, max_depth=12, min_samples_split=5,
        class_weight="balanced", random_state=42
    ),
    "Gradient Boosting": GradientBoostingClassifier(
        n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42
    ),
    "XGBoost": XGBClassifier(
        n_estimators=400, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8,
        eval_metric="logloss", random_state=42
    ),
    "CatBoost": CatBoostClassifier(
        iterations=400, learning_rate=0.05, depth=6,
        verbose=0, random_state=42
    ),
    "Decision Tree": DecisionTreeClassifier(
        max_depth=8, min_samples_split=10,
        class_weight="balanced", random_state=42
    ),
    "KNN": KNeighborsClassifier(
        n_neighbors=7, weights="distance"
    ),
    "SVM": SVC(
        C=1.0, kernel="rbf", probability=True,
        class_weight="balanced", random_state=42
    ),
    "Naive Bayes": GaussianNB()
}

# ------------------------
# Train & evaluate models
# ------------------------
results = []

for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train_scaled, y_train)

    # Predict
    y_pred = model.predict(X_test_scaled)

    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
    else:
        y_prob = model.decision_function(X_test_scaled)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_prob)

    results.append([name, acc, prec, rec, f1, auc])

# ------------------------
# Results table
# ------------------------
results_df = pd.DataFrame(
    results, columns=["Model", "Accuracy", "Precision", "Recall", "F1-Score", "ROC-AUC"]
)
results_df = results_df.sort_values(by="ROC-AUC", ascending=False).reset_index(drop=True)

print("\nModel Comparison Table:")
print(results_df)

"""After Tuning"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd
import numpy as np

# ------------------------
# Parameter grids
# ------------------------
param_grids = {
    "Logistic Regression": {
        "C": [0.01, 0.1, 1, 10, 100],
        "solver": ["lbfgs", "saga"],
        "penalty": ["l2"]
    },
    "Random Forest": {
        "n_estimators": [200, 500, 1000],
        "max_depth": [6, 10, 15, None],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 5],
        "max_features": ["sqrt", "log2"]
    },
    "Gradient Boosting": {
        "n_estimators": [200, 300, 500],
        "learning_rate": [0.01, 0.05, 0.1],
        "max_depth": [3, 5, 7]
    },
    "XGBoost": {
        "n_estimators": [200, 400, 800],
        "learning_rate": [0.01, 0.05, 0.1],
        "max_depth": [3, 5, 7],
        "subsample": [0.6, 0.8, 1.0],
        "colsample_bytree": [0.6, 0.8, 1.0]
    },
    "CatBoost": {
        "iterations": [200, 400, 800],
        "learning_rate": [0.01, 0.05, 0.1],
        "depth": [4, 6, 8],
        "l2_leaf_reg": [1, 3, 5, 7]
    },
    "Decision Tree": {
        "max_depth": [4, 6, 8, 12],
        "min_samples_split": [2, 5, 10, 20],
        "min_samples_leaf": [1, 5, 10]
    },
    "KNN": {
        "n_neighbors": [3, 5, 7, 9, 15],
        "weights": ["uniform", "distance"]
    },
    "SVM": {
        "C": [0.1, 1, 10],
        "kernel": ["rbf", "poly"],
        "gamma": ["scale", "auto"]
    },
    # Naive Bayes has no major hyperparams
    "Naive Bayes": {}
}

# ------------------------
# Base models
# ------------------------
models = {
    "Logistic Regression": LogisticRegression(class_weight="balanced", max_iter=2000, random_state=42),
    "Random Forest": RandomForestClassifier(class_weight="balanced", random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "XGBoost": XGBClassifier(eval_metric="logloss", random_state=42, use_label_encoder=False),
    "CatBoost": CatBoostClassifier(verbose=0, random_state=42),
    "Decision Tree": DecisionTreeClassifier(class_weight="balanced", random_state=42),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(probability=True, class_weight="balanced", random_state=42),
    "Naive Bayes": GaussianNB()
}

# ------------------------
# Train & evaluate with tuning
# ------------------------
results = []

for name, model in models.items():
    print(f"\nTuning {name}...")
    params = param_grids[name]

    if params:  # if there are hyperparameters to tune
        search = RandomizedSearchCV(
            model, param_distributions=params,
            n_iter=10, scoring="roc_auc", cv=3,
            random_state=42, n_jobs=-1, verbose=1
        )
        search.fit(X_train_scaled, y_train)
        best_model = search.best_estimator_
        print(f"Best params for {name}: {search.best_params_}")
    else:
        # For Naive Bayes, just fit directly
        best_model = model.fit(X_train_scaled, y_train)

    # Predict
    y_pred = best_model.predict(X_test_scaled)
    if hasattr(best_model, "predict_proba"):
        y_prob = best_model.predict_proba(X_test_scaled)[:, 1]
    else:
        y_prob = best_model.decision_function(X_test_scaled)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_prob)

    results.append([name, acc, prec, rec, f1, auc])

# ------------------------
# Results table
# ------------------------
results_df = pd.DataFrame(
    results, columns=["Model", "Accuracy", "Precision", "Recall", "F1-Score", "ROC-AUC"]
)
results_df = results_df.sort_values(by="ROC-AUC", ascending=False).reset_index(drop=True)

print("\nModel Comparison Table (Tuned):")
print(results_df)

"""More Tuning, CV, Check for overfitting, Confusion MATRIX & heat map for top 3 models"""

from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# ------------------------
# Dictionary to store results
# ------------------------
results = {}

for name, model in models.items():
    print(f"\nTuning {name}...")
    params = param_grids[name]

    if params:  # Hyperparameter tuning
        search = RandomizedSearchCV(
            model, param_distributions=params,
            n_iter=10, scoring="roc_auc", cv=3,
            random_state=42, n_jobs=-1, verbose=0
        )
        search.fit(X_train_scaled, y_train)
        best_model = search.best_estimator_
        print(f"Best params for {name}: {search.best_params_}")
    else:  # Naive Bayes
        best_model = model.fit(X_train_scaled, y_train)

    # Train & test predictions
    y_train_pred = best_model.predict(X_train_scaled)
    y_test_pred = best_model.predict(X_test_scaled)

    if hasattr(best_model, "predict_proba"):
        y_train_prob = best_model.predict_proba(X_train_scaled)[:, 1]
        y_test_prob = best_model.predict_proba(X_test_scaled)[:, 1]
    else:
        y_train_prob = best_model.decision_function(X_train_scaled)
        y_test_prob = best_model.decision_function(X_test_scaled)

    # CV performance
    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring="roc_auc")
    cv_mean, cv_std = cv_scores.mean(), cv_scores.std()

    # Metrics
    results[name] = {
        "Train AUC": roc_auc_score(y_train, y_train_prob),
        "Test AUC": roc_auc_score(y_test, y_test_prob),
        "CV ROC-AUC Mean": cv_mean,
        "CV ROC-AUC Std": cv_std,
        "Accuracy": accuracy_score(y_test, y_test_pred),
        "Precision": precision_score(y_test, y_test_pred, zero_division=0),
        "Recall": recall_score(y_test, y_test_pred, zero_division=0),
        "F1-Score": f1_score(y_test, y_test_pred, zero_division=0),
        "ROC-AUC": roc_auc_score(y_test, y_test_prob),
        "Model": best_model
    }

# ------------------------
# Results DataFrame
# ------------------------
results_df = pd.DataFrame(results).T.reset_index().rename(columns={"index": "Model"})
results_df = results_df.sort_values(by="ROC-AUC", ascending=False).reset_index(drop=True)

print("\nModel Comparison Table (with CV + Train/Test AUC):")
display(results_df[[
    "Model", "Train AUC", "Test AUC", "CV ROC-AUC Mean", "CV ROC-AUC Std",
    "Accuracy", "Precision", "Recall", "F1-Score", "ROC-AUC"
]])

"""Confusion Matrix for the best model"""

# Get top 3 models by ROC-AUC
top3 = sorted(results.items(), key=lambda x: x[1]['ROC-AUC'], reverse=True)[:3]
top3_names = [x[0] for x in top3]
print("Top 3 models:", top3_names)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

for model_name in top3_names:
    model = results[model_name]["Model"]
    y_pred = model.predict(X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

"""CatBoost interpretability with SHAP"""

import shap

# 1. Define feature names (you already have this list)
feature_names = numeric_cols

# 2. Convert arrays back into DataFrames with proper column names
X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)
X_val_df   = pd.DataFrame(X_val_scaled,   columns=feature_names)
X_test_df  = pd.DataFrame(X_test_scaled,  columns=feature_names)

# Example with CatBoost
best_model = results["CatBoost"]["Model"]

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test_df)

# Summary plot
shap.summary_plot(shap_values, X_test_df)

# Dependence plot for top feature
top_feature = feature_names[0]  # or pick any important one
shap.dependence_plot(top_feature, shap_values, X_test_df)

"""Project-level default probabilities"""

# ------------------------
# Predict project-level default probability using CatBoost
# ------------------------
from catboost import CatBoostClassifier
import pandas as pd
import numpy as np

# Get the trained CatBoost model from your results dictionary
catboost_model = results["CatBoost"]["Model"]

# Features used for prediction
X_all = merged_df[numeric_cols].values
X_all_scaled = scaler.transform(imputer.transform(X_all))  # Ensure same preprocessing

# Predict probability of default (class 1)
merged_df['default_prob'] = catboost_model.predict_proba(X_all_scaled)[:, 1]

# Aggregate probabilities per project
project_default_probs = merged_df.groupby('project_name')['default_prob'].mean().reset_index()

# Sort projects by highest default probability
project_default_probs = project_default_probs.sort_values(by='default_prob', ascending=False)

# Display top 30 highest-risk projects
print("\nTop 30 projects by predicted default probability:")
print(project_default_probs.head(30))

# Optional: barplot of top 30 projects
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,4))
sns.barplot(
    x='default_prob', y='project_name',
    data=project_default_probs.head(30),
    palette='Reds_r'
)
plt.xlabel("Predicted Default Probability")
plt.ylabel("Project Name")
plt.title("Top 30 Highest-Risk Projects - CatBoost Predictions")
plt.xlim(0,1)
plt.show()

import pandas as pd

# Show all rows
pd.set_option('display.max_rows', None)

# Print the full DataFrame
print(project_default_probs)

# (Optional) Reset to default later
pd.reset_option('display.max_rows')

"""Duplicates are there for each projcet"""

import pandas as pd

# Count duplicates
duplicate_counts = merged_df['project_name'].value_counts()
duplicates = duplicate_counts[duplicate_counts > 1]

# Show all rows without truncation
pd.set_option('display.max_rows', None)
print(duplicates)
pd.reset_option('display.max_rows')  # Reset to default

"""Find the names of all the projects"""

#find total project count (not necessary unique)
print("Total projects:", len(merged_df["project_name"].dropna()))

"""All unique project names"""

all_projects = merged_df["project_name"].dropna().unique()
print("Total projects:", len(all_projects))
print(all_projects)

"""Making Sectors"""

def sector(name: str) -> str:
    n = str(name).upper()

    # --- Transport & Infrastructure ---
    if any(w in n for w in ["ROAD", "HIGHWAY", "RAIL", "TRANSPORT", "LOGISTICS", "CORRIDOR", "EDFC", "MITP"]):
        return "Transport & Infrastructure"

    # --- Energy & Power ---
    if any(w in n for w in ["POWER", "ENERGY", "SOLAR", "ELECTRIC", "DISTRIBUTION", "24X7"]):
        return "Energy & Power"

    # --- Water & Irrigation ---
    if any(w in n for w in ["WATER", "IRRIGATION", "DAM", "HYDRO", "BASIN", "WASSIP", "WBADMIP", "KSWMP", "DRIP", "KARN URB WTR", "WTR",  "APIIATP", "SHWSSP" ]):
        return "Water & Irrigation"

    # --- Urban Development & Housing ---
    if any(w in n for w in ["URBAN", "CITY", "HOUSING", "MUNICIPAL", "TOURISM", "TNHHDP", "AMARAVATI", "SWACHH BHARAT"]):
        return "Urban Development & Housing"

    # --- Agriculture & Rural Development ---
    if any(w in n for w in ["AGRI", "FARM", "RURAL", "LIVELIHOOD", "DAIRY", "FISHERIES", "COOPERATIVE", "JOHAR", "POCRA", "CHIRAAG", "TNRTP", "IAMP"]):
        return "Agriculture & Rural Development"

    # --- Health & Social Protection ---
    if any(w in n for w in ["HEALTH", "RSHDP", "COVID", "NUTRITION", "DISABILITY", "HSSP", "ICDS", "TB", "SNGRBP", "PHSPP", "EHSDP", "FSPP", "RESPONSIVE SOCIAL PROTECTION"]):
        return "Health & Social Protection"

    # --- Education & Skills ---
    if any(w in n for w in ["EDUCATION", "SCHOOL", "TRAINING", "SKILL", "UNIVERSITY", "WORKFORCE", "NAHEP", "STARS", "GOAL"]):
        return "Education & Skills"

    # --- Finance & Industry ---
    if any(w in n for w in ["FINANCE", "MSME", "CREDIT", "BANK", "FISCAL", "PFM", "SAL", "DPL", "PFORR", "RAMP", "BFAIR", "PRIVATE FINANCING"]):
        return "Finance & Industry"

    # --- Governance & Policy Reform ---
    if any(w in n for w in ["ISGPP", "CAPABILITY", "GOVERNANCE", "SERVICE DELIVERY", "PSCEP", "SRESTHA", "G-ACRP", "CCP", "UCRRFP", "U-PREPARE"]):
        return "Governance & Policy Reform"

    # --- Environment & Climate ---
    if any(w in n for w in ["CLIMATE", "RESILIENT", "LANDSCAPE", "REWARD", "CHALK", "ASSIST", "SMART", "IPSS-CRRA", "AIRBMP", "TRESP"]):
        return "Environment & Climate"

    # --- Technology & Innovation ---
    if any(w in n for w in ["INNOVATE", "TECH", "ICT", "DIGITAL", "SYSTEMS", "INCLUSION", "ASPIRE", "NECTAR", "INSPIRES", "DAKSH", "KERA"]):
        return "Technology & Innovation"

    # --- Disaster Recovery & Emergency ---
    if any(w in n for w in ["DISASTER", "RECOVERY", "REHABILITATION", "RELIEF"]):
        return "Disaster Recovery & Emergency"

    return "Others"

"""Finding total number of projects in each sector"""

merged_df["sector"] = merged_df["project_name"].apply(sector)
# See sector distribution
print(merged_df["sector"].value_counts())

"""Average Default Probability per Sector"""

sector_analysis = merged_df.groupby('sector').agg(
    unique_projects=('project_name', 'nunique'),       # unique projects
    total_entries=('project_name', 'count'),           # all rows
    avg_default_prob=('default_prob', 'mean')          # average default probability
).reset_index().sort_values(by='avg_default_prob', ascending=False)

print(sector_analysis)

"""High-Risk Projects per Sector"""

# Define high-risk threshold
high_risk_threshold = 0.75

# Count high-risk projects per sector (using unique project names)
high_risk_per_sector = merged_df[merged_df['default_prob'] >= high_risk_threshold] \
    .groupby('sector')['project_name'].nunique().reset_index()

high_risk_per_sector.rename(columns={'project_name': 'high_risk_projects'}, inplace=True)

# Merge with total projects and avg default probability
sector_analysis = merged_df.groupby('sector').agg(
    total_projects=('project_name', 'nunique'),
    avg_default_prob=('default_prob', 'mean')
).reset_index()

sector_analysis = sector_analysis.merge(high_risk_per_sector, on='sector', how='left')
sector_analysis['high_risk_projects'] = sector_analysis['high_risk_projects'].fillna(0).astype(int)

# Sort by avg_default_prob descending
sector_analysis = sector_analysis.sort_values(by='avg_default_prob', ascending=False)

print(sector_analysis)

"""Visualization: Bar Chart of Average Default Probability by Sector"""

import matplotlib.pyplot as plt
import seaborn as sns

# Sort sectors by average default probability
sector_analysis_sorted = sector_analysis.sort_values('avg_default_prob', ascending=False)

plt.figure(figsize=(12,8))
sns.barplot(
    x='avg_default_prob',
    y='sector',
    data=sector_analysis_sorted,
    palette='Reds_r'
)

# Annotate bar values
for index, row in sector_analysis_sorted.iterrows():
    plt.text(
        row['avg_default_prob'] + 0.01,  # slightly after the bar
        index,
        f"{row['avg_default_prob']:.2f}",
        color='black',
        va='center'
    )

plt.xlim(0, 1)
plt.xlabel("Average Default Probability", fontsize=14)
plt.ylabel("Sector", fontsize=14)
plt.title("Average Default Probability by Sector", fontsize=16, weight='bold')
plt.tight_layout()
plt.show()

""" Pie Chart of Project Distribution by Sector"""

import matplotlib.pyplot as plt

# Only include sectors with at least 1 high-risk project
high_risk_data = sector_analysis[sector_analysis['high_risk_projects'] > 0]

# Explode the largest slice for emphasis
explode = [0.08 if x == high_risk_data['high_risk_projects'].max() else 0 for x in high_risk_data['high_risk_projects']]

# Choose a vibrant color palette
colors = plt.cm.Set3.colors  # pastel and visually pleasing

plt.figure(figsize=(10, 8))
plt.pie(
    high_risk_data['high_risk_projects'],
    labels=high_risk_data['sector'],
    autopct=lambda p: f'{p:.1f}% ({int(p * sum(high_risk_data["high_risk_projects"]) / 100)})',
    startangle=140,
    colors=colors,
    explode=explode,
    shadow=True,
    wedgeprops={'edgecolor': 'white', 'linewidth': 1.5},
    textprops={'fontsize': 12, 'weight': 'bold'}
)

plt.title('Distribution of High-Risk Projects by Sector', fontsize=18, fontweight='bold')
plt.axis('equal')
plt.tight_layout()
plt.show()

"""ECL (BEFORE STRESS)

EAD(EXposure at Default)
"""

#Finding EAD(Exposure at default)
# Set conversion rate (USD to INR)
usd_to_inr = 83

# Sort by project and date
merged_df = merged_df.sort_values(by=['project_name', 'end_of_period'])
# Drop rows where borrower obligation is NaN (needed for accurate EAD)
merged_df = merged_df.dropna(subset=['project_name','borrowers_obligation_ususd'])


# Find the latest date per project
latest_dates = merged_df.groupby('project_name')['end_of_period'].max().reset_index()

# Keep only loans from the latest date
latest_loans = pd.merge(
    merged_df,
    latest_dates,
    on=['project_name', 'end_of_period'],
    how='inner'
)

# Weighted PD per project: sum(EAD * default_prob) / sum(EAD)
pd_weighted = latest_loans.groupby('project_name').apply(
    lambda x: pd.Series({
        'PD': (x['borrowers_obligation_ususd'] * usd_to_inr * x['default_prob']).sum() /
              (x['borrowers_obligation_ususd'] * usd_to_inr).sum()
    })
).reset_index()

# Display weighted PD per project
print(pd_weighted.head(20))

#Takes the latest active loan of projects and if any projject has 2,3 active loans it sums up their ead
ead_summary = (
    latest_loans.groupby('project_name', as_index=False)
    .agg(
        total_active_loans=('loan_number', 'count'),  # how many loans active
        EAD_INR=('borrowers_obligation_ususd', lambda x: (x.sum() * usd_to_inr))  # sum & convert to INR
    )
)

# Sort by largest exposures
ead_summary = ead_summary.sort_values('EAD_INR', ascending=False)

# Show top 20 exposures
print(ead_summary.head(20))

#Plots
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming ead_summary is already created as you mentioned

# Top 20 projects by EAD
top_ead = ead_summary.head(20)

# Bar chart
plt.figure(figsize=(12,6))
sns.barplot(data=top_ead, x='EAD_INR', y='project_name', palette='viridis')
plt.xlabel('EAD (INR)')
plt.ylabel('Project Name')
plt.title('Top 20 Projects by Exposure at Default (EAD)')
plt.tight_layout()
plt.show()

# Pie chart for top 10 exposures
top_ead_pie = top_ead.head(10)
plt.figure(figsize=(8,8))
plt.pie(top_ead_pie['EAD_INR'], labels=top_ead_pie['project_name'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('viridis', 10))
plt.title('Top 10 Projects by EAD Distribution')
plt.show()

# --- Top Projects by EAD ---
top_ead_projects = ead_summary.sort_values("EAD_INR", ascending=False).head(10)
print("Top 10 Projects by Exposure at Default (EAD):")
print(top_ead_projects[['project_name', 'EAD_INR']])

"""LGD(Loss given Default)"""

defaulted_loans = merged_df[merged_df['default_flag'] == 1]

#taken the non default loans as well
# Set conversion rate
usd_to_inr = 83

# Convert USD columns to INR
merged_df['borrowers_obligation_inr'] = merged_df['borrowers_obligation_ususd'] * usd_to_inr
merged_df['repaid_to_ibrd_inr'] = merged_df['repaid_to_ibrd_ususd'] * usd_to_inr

# Calculate LGD for all loans
merged_df['LGD'] = ((merged_df['borrowers_obligation_inr'] - merged_df['repaid_to_ibrd_inr'])
                     / merged_df['borrowers_obligation_inr'])

# For performing loans (not defaulted), LGD is 0
merged_df.loc[merged_df['default_flag'] == 0, 'LGD'] = 0

# Aggregate EAD and LGD per project
lgd_all_projects = merged_df.groupby('project_name', as_index=False).apply(
    lambda x: pd.Series({
        'EAD_INR': x['borrowers_obligation_inr'].sum(),
        'LGD': ((x['borrowers_obligation_inr'] * x['LGD']).sum()) / x['borrowers_obligation_inr'].sum()
    })
).reset_index(drop=True)

# Sort by LGD or EAD if needed
lgd_all_projects = lgd_all_projects.sort_values('EAD_INR', ascending=False)

# Show output
print(lgd_all_projects[['project_name', 'EAD_INR', 'LGD']])

# Assuming lgd_all_projects is all loans
#lgd of all projectsa
top_lgd_all = lgd_all_projects.sort_values('LGD', ascending=False).head(20)

# Bar chart
plt.figure(figsize=(12,6))
sns.barplot(data=top_lgd_all, x='LGD', y='project_name', palette='coolwarm')
plt.xlabel('LGD')
plt.ylabel('Project Name')
plt.title('Top 20 Projects by LGD (All Loans)')
plt.tight_layout()
plt.show()

top_lgd_projects = lgd_all_projects.sort_values("LGD", ascending=False).head(10)
print("\nTop 10 Projects by Loss Given Default (LGD):")
print(top_lgd_projects[['project_name', 'LGD']])

# --- Step 1: Prepare PD dataframe (using your variable project_default_probs) ---
pd_df = pd.DataFrame({
    "project_name": pd_weighted['project_name'],
    "PD": pd_weighted['PD']
})

# --- Step 2: Merge EAD, LGD, and PD ---
ecl_df = (ead_summary[['project_name', 'EAD_INR']]
          .merge(lgd_all_projects[['project_name', 'LGD']], on='project_name', how='left')
          .merge(pd_df, on='project_name', how='left'))

# --- Step 3: Calculate ECL ---
ecl_df['ECL_INR'] = ecl_df['EAD_INR'] * ecl_df['LGD'] * ecl_df['PD']

# --- Step 4: Sort by highest ECL ---
ecl_df = ecl_df.sort_values('ECL_INR', ascending=False)

# Show top 20
print(ecl_df[['project_name', 'EAD_INR', 'LGD', 'PD', 'ECL_INR']].head(20))

import matplotlib.pyplot as plt

# Pick top 15 projects
plot_df = ecl_df.sort_values("ECL_INR", ascending=False).head(15)

plt.figure(figsize=(12,6))
plt.barh(plot_df["project_name"], plot_df["ECL_INR"]/1e9, color="tomato")  # scale to billions
plt.gca().invert_yaxis()

plt.title("Top 15 Projects by Expected Credit Loss (ECL)", fontsize=14)
plt.xlabel("ECL (INR Billion)")
plt.ylabel("Project")
plt.tight_layout()
plt.show()

# Drop any old LGD, PD, EAD, or ECL columns to avoid conflicts
cols_to_drop = ['LGD', 'LGD_x', 'LGD_y', 'PD', 'EAD_INR', 'ECL_INR']
merged_df = merged_df.drop(columns=[c for c in cols_to_drop if c in merged_df.columns])

# Now merge the new columns from ecl_df
merged_df = merged_df.merge(
    ecl_df[['project_name', 'EAD_INR', 'PD', 'LGD', 'ECL_INR']],
    on='project_name',
    how='left'
)

# checking ecl for any project
project = merged_df[merged_df['project_name'] == 'India COVID-19 ERHSP Project']

# Get latest values for EAD, PD, LGD
latest = project.sort_values('end_of_period').iloc[-1]

ead = latest['EAD_INR']
pd = latest['PD']
lgd = latest['LGD']

# Calculate ECL
ecl = ead * pd * lgd

print(f"EAD: {ead} Billion INR")
print(f"PD: {pd}")
print(f"LGD: {lgd}")
print(f"Calculated ECL: {ecl} Billion INR")

"""STRESS TESTING"""

# === 1) Build yearly aggregation (portfolio PD) and attach macro vars ===
import pandas as pd
import numpy as np
import statsmodels.api as sm

# Use merged_df from your script. It should already contain:
#  - 'year' (int)
#  - 'default_flag' (0/1)  OR 'default_prob' (model PD per loan)
#  - 'gdp_growth' and 'cpi_inflation' (from stress_long merge)

# 1A: Create yearly aggregates
yearly = merged_df.groupby('year').agg(
    pd_obs = ('default_flag', 'mean'),         # observed default rate in that year
    pd_pred = ('default_prob', 'mean'),        # average model PD (if available)
    gdp = ('gdp_growth', 'mean'),
    cpi = ('cpi_inflation', 'mean')
).reset_index()

print("Yearly head:\n", yearly.head())

# === 2) Standardize macro units to *percentage points* (1 == 1 percentage point) ===
# If values look like decimals (e.g. 0.02), convert to pct *100; if already 2 or -1.5, keep as-is.
for col in ['gdp','cpi']:
    if yearly[col].abs().max() <= 1.5:   # likely decimals (0.02 etc.)
        yearly[col + '_pp'] = yearly[col] * 100.0
    else:
        yearly[col + '_pp'] = yearly[col]

print("\nAfter unit check (gdp_pp, cpi_pp):\n", yearly[['year','gdp','gdp_pp','cpi','cpi_pp']].head())

# === 3) Choose target PD series for regression ===
# Prefer observed default rate (pd_obs) when you have enough years/samples.
# If pd_obs is very noisy (few defaults), use pd_pred (average model PD).
# Here we'll compute both regressions and compare.
yearly = yearly.dropna(subset=['gdp_pp','cpi_pp']).sort_values('year')  # keep years with macro

# Helper to run OLS and print concise results
def run_beta_regression(df, target_col):
    X = df[['gdp_pp','cpi_pp']]
    X = sm.add_constant(X)    # intercept
    y = df[target_col]
    model = sm.OLS(y, X).fit(cov_type='HC3')   # robust se (HC3)
    print(f"\nRegression results for target = {target_col}:\n")
    print(model.summary())
    # extract betas (per 1 percentage point change)
    beta_gdp = model.params.get('gdp_pp', np.nan)
    beta_cpi = model.params.get('cpi_pp', np.nan)
    return model, beta_gdp, beta_cpi

# Run on observed default rate (preferred if not too sparse)
model_obs, beta_gdp_obs, beta_cpi_obs = run_beta_regression(yearly, 'pd_obs')

# Also run on mean model PD (pd_pred) if available
if 'pd_pred' in yearly.columns and not yearly['pd_pred'].isnull().all():
    model_pred, beta_gdp_pred, beta_cpi_pred = run_beta_regression(yearly, 'pd_pred')
else:
    model_pred, beta_gdp_pred, beta_cpi_pred = (None, np.nan, np.nan)

"""Short takeaway

The regression shows no reliable relationship between your yearly PDs and GDP/CPI. The estimated sensitivities (betas) are extremely small and statistically insignificant, so you should not use them as-is for stress testing.

1) PREPARING FOR STRESS TESTING
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm

# === 1) Prepare loan-level dataset ===
# Assumptions: merged_df already has
#   - 'pd_pred' (continuous predicted PD per loan, e.g. from your credit model)
#   - 'gdp' and 'cpi' (annual values, already merged in earlier step)
#   - 'year'
#   - 'EAD' (exposure at default)
#   - optional: 'LGD' (loss given default), else we set a fixed value

# Convert to percentage points if not already
for col in ['gdp_growth','cpi_inflation']:
    if merged_df[col].abs().max() <= 1.5:   # e.g., 0.07 = 7%
        merged_df[col + '_pp'] = merged_df[col] * 100
    else:
        merged_df[col + '_pp'] = merged_df[col]

# Define LGD if not present
if 'LGD' not in merged_df.columns:
    LGD_default = 0.45
    merged_df['LGD'] = LGD_default

# Make sure EAD exists
if 'EAD' not in merged_df.columns:
    if 'disbursed_amount_ususd' in merged_df.columns:
        merged_df['EAD'] = merged_df['disbursed_amount_ususd'].fillna(0)
    else:
        merged_df['EAD'] = 0.0

# === 2) Loan-level regression of PD on GDP & CPI ===
X = merged_df[['gdp_growth_pp','cpi_inflation_pp']]
X = sm.add_constant(X)
y = merged_df['default_prob']   # continuous PD per loan

model = sm.OLS(y, X).fit(cov_type='HC3')
print(model.summary())

# Extract betas
beta_gdp = model.params.get('gdp_growth_pp')
beta_cpi = model.params.get('cpi_inflation_pp')
print(f"\nEstimated betas: GDP = {beta_gdp:.4f}, CPI = {beta_cpi:.4f}")

"""This means GDP and CPI explain almost none of the variation in loan-level PDs. Most of the variation comes from other factors (sector, borrower, loan structure, etc.).

--> But this regression is still very useful for stress testing, because the betas (0.0013 and ‚Äì0.0075) tell you how much PD would shift if you shock GDP or CPI.

Run regression (PD ~ GDP + CPI)

We used OLS regression to see how PD reacts when GDP or CPI moves.

We did this at two levels:

Yearly portfolio PDs (pd_obs or pd_pred)

Loan-level PDs (default_prob for each loan)

The coefficients (betas) tell us sensitivity:

Example: Œ≤_GDP = 0.0013 ‚Üí if GDP falls by 1pp, PD increases by 0.0013.

Œ≤_CPI = ‚Äì0.0075 ‚Üí if inflation rises by 1pp, PD decreases by 0.0075.
(In reality, we flip signs to match intuition: bad economy = higher PDs.)

2) Apply stress scenarios

üîπ Step A: Define a stress scenario

For example:

GDP growth falls by ‚Äì3%

CPI inflation rises by +2%
"""

delta_gdp = -3.0
delta_cpi = +2.0

"""üîπ Step B: Apply the regression formula

Your regression equation is:

PD
ùë†
ùë°
ùëü
ùëí
ùë†
ùë†
ùëí
ùëë
=
PD
ùëè
ùëé
ùë†
ùëí
ùëô
ùëñ
ùëõ
ùëí
+
ùõΩ
ùê∫
ùê∑
ùëÉ
√ó
Œî
ùê∫
ùê∑
ùëÉ
+
ùõΩ
ùê∂
ùëÉ
ùêº
√ó
Œî
ùê∂
ùëÉ
ùêº
PD
stressed
	‚Äã

=PD
baseline
	‚Äã

+Œ≤
GDP
	‚Äã

√óŒîGDP+Œ≤
CPI
	‚Äã

√óŒîCPI

PROJECT LEVEL DEFAULT AFTER STRESS TESTING
"""

# Assume you already estimated betas from regression:
# beta_gdp, beta_cpi
# And you have GDP and CPI shock values in % points

gdp_shock = -3   # example: -2 percentage points
cpi_shock = 2   # example: +1 percentage point

# --- Step A: Baseline PD ---
merged_df['pd_base'] = merged_df['default_prob']   # your original probability column

# --- Step B: Apply shocks to create stressed PD ---
merged_df['pd_stressed'] = (
    merged_df['pd_base'] +
    beta_gdp * gdp_shock +
    beta_cpi * cpi_shock
)

# Ensure valid probability range
merged_df['pd_stressed'] = merged_df['pd_stressed'].clip(0, 1)

import matplotlib.pyplot as plt
import seaborn as sns

# Prepare plotting data: one row per project, just for visualization
df_plot = merged_df.groupby("project_name").first()[["default_prob", "pd_stressed"]].reset_index()

# Sort by stressed PD for readability
df_plot = df_plot.sort_values("pd_stressed", ascending=True)

# Set style
sns.set(style="whitegrid")

# Plot horizontal bars
plt.figure(figsize=(12, max(6, 0.3*len(df_plot))))  # dynamic height
plt.barh(df_plot["project_name"], df_plot["default_prob"], color="#4c72b0", label="Baseline PD")
plt.barh(df_plot["project_name"], df_plot["pd_stressed"]-df_plot["default_prob"],
         left=df_plot["default_prob"], color="#dd8452", label="Stressed Increase")

plt.xlabel("Probability of Default (PD)")
plt.ylabel("Project Name")
plt.title("Baseline vs Stressed PD per Project")
plt.legend()
plt.tight_layout()
plt.show()

"""SECTORAL DEFAULT AFTER STRESS TESTING"""

# Step 1: Aggregate
sector_stress_analysis = merged_df.groupby('sector').agg(
    unique_projects=('project_name', 'nunique'),
    avg_default_prob=('default_prob', 'mean'),       # baseline average PD
    avg_pd_stressed=('pd_stressed', 'mean')          # stressed average PD
).reset_index()

# Step 2: Compute the shift in PD
sector_stress_analysis["change_in_pd"] = (
    sector_stress_analysis["avg_pd_stressed"] - sector_stress_analysis["avg_default_prob"]
)

# Sort by stressed PD
sector_stress_analysis = sector_stress_analysis.sort_values(by="avg_pd_stressed", ascending=False)

print(sector_stress_analysis)

import matplotlib.pyplot as plt
import numpy as np

# Sort sectors by stressed PD for better visual
sector_stress_analysis_sorted = sector_stress_analysis.sort_values('avg_pd_stressed', ascending=False)

sectors = sector_stress_analysis_sorted['sector']
baseline = sector_stress_analysis_sorted['avg_default_prob']
stressed = sector_stress_analysis_sorted['avg_pd_stressed']

x = np.arange(len(sectors))
width = 0.35  # width of bars

plt.figure(figsize=(14, 8))
plt.bar(x - width/2, baseline, width, label='Baseline PD', color='#4C72B0')
plt.bar(x + width/2, stressed, width, label='Stressed PD', color='#DD8452')

plt.xticks(x, sectors, rotation=45, ha='right')
plt.ylabel('Average Probability of Default')
plt.title('Sectoral Average PD: Baseline vs Stressed')
plt.legend()
plt.tight_layout()
plt.show()

merged_df[["gdp_growth", "gdp_growth_pp", "cpi_inflation", "cpi_inflation_pp"]]

# Only multiply if values are decimals < 1.5
for col in ['gdp_growth','cpi_inflation']:
    if merged_df[col].abs().max() <= 1.5:
        merged_df[col + '_pp'] = merged_df[col] * 100
    else:
        merged_df[col + '_pp'] = merged_df[col]  # keep as-is

"""ECL (UNDER STRESS)"""

import numpy as np
import matplotlib.pyplot as plt

# --- Formatter function ---
def human_readable(num):
    """Convert INR values to human-readable words (Billion, Million)."""
    if num >= 1e9:
        return f"{num/1e9:.0f} Billion"
    elif num >= 1e6:
        return f"{num/1e6:.0f} Million"
    elif num >= 1e3:
        return f"{num/1e3:.0f} Thousand"
    else:
        return f"{num:.0f}"

# --- Step 5: Show summary table (formatted) ---
ecl_compare_display = ecl_compare_df.copy()
for col in ['EAD_INR','ECL_Base_INR','ECL_Stressed_INR','Change_INR']:
    ecl_compare_display[col] = ecl_compare_display[col].apply(human_readable)

print("\nTop 20 Projects - Before vs After Stress Testing\n")
print(
    ecl_compare_display[['project_name','EAD_INR','LGD','pd_base','ECL_Base_INR',
                         'pd_stressed','ECL_Stressed_INR','Change_INR','Change_%']].head(20)
)

# --- Step 6: Visualization ---
plot_df = ecl_compare_df.head(15)

plt.figure(figsize=(14,7))
bar_width = 0.35
index = np.arange(len(plot_df))

# Baseline bars
plt.barh(index - bar_width/2,
         plot_df['ECL_Base_INR']/1e9,
         bar_width, label="Baseline ECL", color='steelblue')

# Stressed bars
plt.barh(index + bar_width/2,
         plot_df['ECL_Stressed_INR']/1e9,
         bar_width, label="Stressed ECL", color='crimson')

plt.yticks(index, plot_df['project_name'])
plt.gca().invert_yaxis()

plt.title("Top 15 Projects: Baseline vs Stressed ECL", fontsize=14)
plt.xlabel("ECL (INR Billion)")
plt.ylabel("Project")
plt.legend()
plt.tight_layout()

# Add value labels in words
for i, (base, stress) in enumerate(zip(plot_df['ECL_Base_INR'], plot_df['ECL_Stressed_INR'])):
    plt.text(base/1e9, i - bar_width/2, human_readable(base), va='center', ha='left', fontsize=8)
    plt.text(stress/1e9, i + bar_width/2, human_readable(stress), va='center', ha='left', fontsize=8)

plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Make sure we have sector info ---
if "sector" in merged_df.columns:
    # sector info already inside merged_df
    project_sector_map = merged_df[['project_name','sector']].drop_duplicates()
elif "sector_mapping" in globals():
    # if you had created a mapping earlier with another name
    project_sector_map = sector_mapping[['project_name','sector']].drop_duplicates()
else:
    raise ValueError("‚ùå Could not find project ‚Üí sector mapping. Please provide a DataFrame with columns ['project_name','sector'].")

# --- Step 2: Merge sector info into ECL Data ---
ecl_sector_df = (
    ecl_compare_df
    .merge(project_sector_map, on='project_name', how='left')
)

# --- Step 3: Aggregate at Sector Level ---
sector_summary = (
    ecl_sector_df
    .groupby('sector')
    .agg({
        'EAD_INR':'sum',
        'ECL_Base_INR':'sum',
        'ECL_Stressed_INR':'sum'
    })
    .reset_index()
)

# --- Step 4: Calculate Change (absolute & %) ---
sector_summary['Change_INR'] = (
    sector_summary['ECL_Stressed_INR'] - sector_summary['ECL_Base_INR']
)
sector_summary['Change_%'] = (
    (sector_summary['Change_INR'] / sector_summary['ECL_Base_INR'].replace(0, np.nan)) * 100
).round(2)

# --- Step 5: Human-readable formatting ---
def human_readable(num):
    if num >= 1e9:
        return f"{num/1e9:.0f} Billion"
    elif num >= 1e6:
        return f"{num/1e6:.0f} Million"
    elif num >= 1e3:
        return f"{num/1e3:.0f} Thousand"
    else:
        return f"{num:.0f}"

sector_display = sector_summary.copy()
for col in ['EAD_INR','ECL_Base_INR','ECL_Stressed_INR','Change_INR']:
    sector_display[col] = sector_display[col].apply(human_readable)

print("\n‚úÖ Sectoral ECL - Before vs After Stress Testing\n")
print(sector_display[['sector','EAD_INR','ECL_Base_INR','ECL_Stressed_INR','Change_INR','Change_%']])

# --- Step 6: Visualization ---
plot_df = sector_summary.sort_values('ECL_Stressed_INR', ascending=False).head(10)

plt.figure(figsize=(14,7))
bar_width = 0.35
index = np.arange(len(plot_df))

# Baseline bars
plt.barh(index - bar_width/2,
         plot_df['ECL_Base_INR']/1e9,
         bar_width, label="Baseline ECL", color='steelblue')

# Stressed bars
plt.barh(index + bar_width/2,
         plot_df['ECL_Stressed_INR']/1e9,
         bar_width, label="Stressed ECL", color='crimson')

plt.yticks(index, plot_df['sector'])
plt.gca().invert_yaxis()

plt.title("Top 10 Sectors: Baseline vs Stressed ECL", fontsize=14)
plt.xlabel("ECL (INR Billion)")
plt.ylabel("Sector")
plt.legend()
plt.tight_layout()

# Add value labels
for i, (base, stress) in enumerate(zip(plot_df['ECL_Base_INR'], plot_df['ECL_Stressed_INR'])):
    plt.text(base/1e9, i - bar_width/2, human_readable(base), va='center', ha='left', fontsize=8)
    plt.text(stress/1e9, i + bar_width/2, human_readable(stress), va='center', ha='left', fontsize=8)

plt.show()